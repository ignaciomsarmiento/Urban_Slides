{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b35ddf9a",
   "metadata": {},
   "source": [
    "# Experiment Designs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0835a1d1",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/ignaciomsarmiento/Urban_Slides/tree/main/Lecture14/Notebook_Power.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40792975",
   "metadata": {},
   "source": [
    "## Experiment Design:  Power Analysis\n",
    "\n",
    "- We want to separate signal from noise.\n",
    "\n",
    "- Two errors that can be made\n",
    "\n",
    "\n",
    "<div >\n",
    "<img src = \"figs/matrix_2.png\" />\n",
    "</div>\n",
    "\n",
    "\n",
    "- Type I error = probability of rejecting the null hypothesis, given true effect $=$ 0.\n",
    "\n",
    "- Power (1- Type II error)= probability of rejecting null hypothesis, given true effect $\\ne$ 0.\n",
    "\n",
    "\n",
    "- Power, in other words, it is the ability to detect an effect given that it exists.\n",
    "\n",
    "- Power analysis is something we do **before** we run a study.\n",
    "\n",
    "  - Helps you figure out the sample you need to detect a given effect size.\n",
    "    \n",
    "  - Or helps you figure out a minimal detectable difference given a set sample size.\n",
    "    \n",
    "  - May help you decide whether to run a study.\n",
    "\n",
    "\n",
    "### Approaches to power calculation\n",
    "\n",
    "\n",
    "    - Analytical calculations of power\n",
    "    - Simulation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f57e07",
   "metadata": {},
   "source": [
    "## Analytical calculations of power\n",
    "\n",
    "- Formula:\n",
    "  \\begin{align*}\n",
    "  \\text{Power} &= \\Phi\\left(\\frac{|\\tau| \\sqrt{N}}{2\\sigma}- \\Phi^{-1}(1- \\frac{\\alpha}{2})\\right)\n",
    "  \\end{align*}\n",
    "\n",
    "- Components:\n",
    "  - $\\Phi$: standard normal CDF is monotonically increasing\n",
    "  - $\\tau$: the effect size\n",
    "  - $N$: the sample size\n",
    "  - $\\sigma$: the standard deviation of the outcome\n",
    "  - $\\alpha$: the significance level (typically 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f34321",
   "metadata": {},
   "source": [
    "### Power Formula Derivation\n",
    "\n",
    "**Power = Probability of detecting a true effect**\n",
    "\n",
    "Under the alternative hypothesis $H_1$, we want to know: \"What's the probability our test statistic exceeds the critical value?\"\n",
    "\n",
    "\n",
    "\n",
    "#### Step-by-Step Derivation\n",
    "\n",
    "**Setup:** Two-sample test comparing treatment vs. control (equal groups of size N/2 each)\n",
    "\n",
    "1. **Under $H_0$** (no effect):\n",
    "   $$\\text{Test statistic: } T = \\frac{\\bar{Y}_{\\text{treat}} - \\bar{Y}_{\\text{control}}}{\\text{SE}} \\sim N(0,1)$$\n",
    "\n",
    "2. **Under $H_1$** (true effect = $\\tau$):\n",
    "   $$T \\sim N\\left(\\frac{\\tau}{\\text{SE}}, 1\\right)$$\n",
    "   \n",
    "   where $\\text{SE} = \\sqrt{\\frac{2\\sigma^2}{N/2}} = \\frac{2\\sigma}{\\sqrt{N}}$\n",
    "\n",
    "3. **Critical value** (two-sided test at level $\\alpha$):\n",
    "   $$c = \\Phi^{-1}\\left(1 - \\frac{\\alpha}{2}\\right)$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Computing Power\n",
    "\n",
    "4. **Power** = P(Reject H₀ | H₁ is true)  \n",
    "   $$= P\\left(T > c \\mid T \\sim N\\left(\\frac{\\tau}{\\text{SE}}, 1\\right)\\right)$$\n",
    "\n",
    "5. **Standardize** the distribution:\n",
    "   $$= P\\left(Z > c - \\frac{\\tau}{\\text{SE}}\\right) \\text{ where } Z \\sim N(0,1)$$\n",
    "\n",
    "6. **Substitute** $\\text{SE} = \\frac{2\\sigma}{\\sqrt{N}}$:\n",
    "   $$\\text{Power} = \\Phi\\left(\\frac{\\tau\\sqrt{N}}{2\\sigma} - \\Phi^{-1}\\left(1-\\frac{\\alpha}{2}\\right)\\right)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda2e63a",
   "metadata": {},
   "source": [
    "\n",
    "#### Limitations to analytical power calculations\n",
    "\n",
    "- Only derived for some test statistics (e.g. differences of means)\n",
    "\n",
    "- Makes specific assumptions about the data-generating process\n",
    "\n",
    "- Incompatible with more complex designs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261716a1",
   "metadata": {},
   "source": [
    "## Simulation-based power calculation\n",
    "\n",
    "- Create dataset and simulate research design.\n",
    "\n",
    "- Assumptions are necessary for simulation studies, but you make your own.\n",
    "\n",
    "- For the DeclareDesign approach, see <https://declaredesign.org/>\n",
    "\n",
    "### Steps\n",
    "\n",
    "\n",
    "  - Model\n",
    "  \n",
    "  - Inquiry\n",
    "  \n",
    "  - Data Strategy\n",
    "     \n",
    "  - Answer Strategy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376c2cf3",
   "metadata": {},
   "source": [
    "#### Practical simulation checklist\n",
    "\n",
    "1. Encode plausible effect sizes, variances, and correlations that reflect the institutional context (e.g., block-level randomization, intra-cluster correlation).\n",
    "2. Simulate the full assignment and measurement process so diagnostics include any imbalance, attrition, or transformation you expect in the field.\n",
    "3. Summarize the simulated estimands with the same estimator you plan to report; power is the share of simulations that reject $H_0$ at your target $\\alpha$.\n",
    "4. Iterate by scaling sample sizes or tightening measurement (adding covariates, stratifying) until the simulated power reaches the desired threshold (often 0.8).\n",
    "5. Archive the code and assumptions with the pre-analysis plan so reviewers can audit exactly how design choices were justified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36adbf5",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "install.packages(\"pacman\")\n",
    "library(\"pacman\")\n",
    "\n",
    "p_load(\"DeclareDesign\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e02c153",
   "metadata": {},
   "source": [
    "## Simple Design "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecae8c55",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "- Models are theoretical abstractions we use to make sense of the world and organize our understanding of it.\n",
    "- Models describe the units, conditions, and outcomes that define inquiries. \n",
    "\n",
    "\n",
    "<div >\n",
    "    <img src = \"figs/figure-6-1.svg\" />\n",
    "</div>\n",
    "\n",
    "- To assess many properties of a research design we often need to make the leap from nonparametric models to parametric structural causal models. \n",
    "- We need to enumerate beliefs about \n",
    "    - effect sizes,\n",
    "    - specific functional forms, \n",
    "    - etc..\n",
    "    \n",
    "Since any particular choice for these parameters could be close or far from the truth, we will typically consider a range of plausible values for each model parameter.\n",
    "\n",
    "One possible parametric model is given by the following:\n",
    "\n",
    "$$\n",
    "Y = 1 \\times Z + U\n",
    "$$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e1b8ea",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "design<-declare_model(\n",
    "        N=40,\n",
    "        U=rnorm(N,sd=1),\n",
    "        potential_outcomes(Y~ 1*Z +U)) +NULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636f4c2e",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "head(draw_data(design),10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a212b677",
   "metadata": {},
   "source": [
    "### Inquiry\n",
    "\n",
    "- An inquiry is a question we ask of the world, and in the same way, of our models of the world. \n",
    "- If we stipulate a reference model, then our inquiry is a summary of the model. \n",
    "- Suppose in some reference model that $Z$ affects $Y$. Inquiries might be: \n",
    "    - Descriptive: what is the average level of $Y$ when $Z=1$ , under the model? \n",
    "    - Causal: what is the average treatment effect of $Z$ on $Y$ ? \n",
    "    - etc.\n",
    "    \n",
    "- Here we defind our **estimand**:\n",
    "    - Estimand: Parameter in the population which is to be estimated in a statistical analysis\n",
    "    - Estimator: A rule for calculating an estimate of a given quantity based on observed data. Function of the observations, i.e., how observations are put together\n",
    "    - Estimation: The process of finding an estimate, or approximation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f31081",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "design<- declare_model(\n",
    "            N=40,\n",
    "            U=rnorm(N,sd=1),\n",
    "            potential_outcomes(Y~ 1*Z +U )) +\n",
    "            declare_inquiry(ATE=Y_Z_1-Y_Z_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558a043f",
   "metadata": {},
   "source": [
    "### Data strategy\n",
    "\n",
    "- Depending on the design, the data strategy could include decisions about any or all of the following: \n",
    "    - sampling: the procedure for selecting which units will be measured \n",
    "    - treatment assignment: procedure for allocating treatments to sampled unit\n",
    "    - measurement: measurement is the procedure for turning information about the sampled units into data.  \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97408fd8",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "design<- declare_model(\n",
    "            N=40,\n",
    "            U=rnorm(N,sd=1),\n",
    "            potential_outcomes(Y~ 1*Z +U )) +\n",
    "            declare_inquiry(ATE=Y_Z_1-Y_Z_0) + \n",
    "            declare_assignment(Z=complete_ra(N,prob=0.5)) +\n",
    "            declare_measurement(Y=reveal_outcomes(Y~Z))\n",
    "\n",
    "head(draw_data(design),10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124ea0c1",
   "metadata": {},
   "source": [
    "### Answer Strategy\n",
    "\n",
    "-  The answer strategy is what we use to summarize the data produced by the data strategy. \n",
    "- Just like the inquiry summarizes a part of the model, the answer strategy summarizes a part of the data. \n",
    "- Answer strategies are functions that take in data and return answers, e.g. `lm`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab3cb2d",
   "metadata": {},
   "source": [
    "#### Estimand vs. estimator vs. estimate\n",
    "\n",
    "- **Estimand (ATE)**: the population quantity implied by the model. In notation,\\n\n",
    "  \\begin{equation*}\\text{ATE} = \\mathbb{E}[Y(1)-Y(0)] = \\frac{1}{N} \\sum_{i=1}^{N} \\left( Y_i(1) - Y_i(0) \\right).\\end{equation*}\n",
    "\n",
    "  In DeclareDesign we encode this with `declare_inquiry(ATE = mean(Y_Z_1 - Y_Z_0))`.\\n\n",
    "- **Estimator (OLS coefficient on $Z$)**: a mapping from observed data to a number. With the specification\n",
    "\n",
    "  \\begin{equation*} Y_i = \\beta_0 + \\beta_1 Z_i + \\varepsilon_i, \\end{equation*}\n",
    "  \n",
    "  the estimator for the ATE is the closed-form OLS coefficient\n",
    "  \n",
    "  \\begin{equation*} \\hat{\\beta}_1 = \\frac{\\sum_i (Z_i-\\bar{Z})(Y_i-\\bar{Y})}{\\sum_i (Z_i-\\bar{Z})^2}, \\end{equation*}\n",
    "\n",
    "  which is implemented through `declare_estimator(Y ~ Z, .method = lm)`.\\n\n",
    "- **Estimate**: the realized value of the estimator in one dataset. In the regression output under the `estimate` column you can see, for example, `1.13` as the sample ATE, accompanied by its standard error, test statistic, and confidence interval.\n",
    "\n",
    "Together: the estimand is the theoretical ATE, the estimator is the linear model rule that targets it, and the estimate is the numeric result produced by applying that rule to simulated or observed data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcfee0a",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "design<- declare_model(\n",
    "            N=40,\n",
    "            U=rnorm(N,sd=1),\n",
    "            potential_outcomes(Y~ 1*Z +U )) +\n",
    "            declare_inquiry(ATE=mean(Y_Z_1-Y_Z_0)) + \n",
    "            declare_assignment(Z=complete_ra(N,prob=0.5)) +\n",
    "            declare_measurement(Y=reveal_outcomes(Y~Z))+\n",
    "            declare_estimator(Y~Z,.method=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c813449c",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "draw_estimates(design)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa84fee2",
   "metadata": {},
   "source": [
    "## Diagnosis\n",
    "\n",
    "- Once a design is declared in code, diagnosing it is usually the easy part. \n",
    "- `diagnose_design` handles almost everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89d079c",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "diagnose_design(design,sims=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12383897",
   "metadata": {},
   "source": [
    "### Redesign\n",
    "\n",
    "- More often, you’ll vary designs over a parameter with redesign to assess your experimental design. \n",
    "- Any quantity that you define in the global environment and use in a declaration step can become a parameter like this and then altered via redesign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6aee5e7",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Define default values first\n",
    "N <- 100  # or whatever default you want\n",
    "ates <- 1\n",
    "\n",
    "# Now create your design using those variables\n",
    "design4 <- declare_model(\n",
    "            N=N,\n",
    "            U=rnorm(N,sd=1),\n",
    "            ates=ates,  # now references the variable ates\n",
    "            potential_outcomes(Y~ ates*Z +U )) + \n",
    "        declare_inquiry(ATE=mean(Y_Z_1-Y_Z_0)) +\n",
    "        declare_assignment(Z=complete_ra(N,prob=0.5)) + \n",
    "        declare_measurement(Y=reveal_outcomes(Y~Z)) +\n",
    "        declare_estimator(Y~Z,term=\"Z\", .method=lm)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8139528",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "multiple_design <- redesign(design4, ates=c(0.5,1,1.5), N=c(20,40,50))\n",
    "diagnose_design(multiple_design, sims=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74dc7b0",
   "metadata": {},
   "source": [
    "## Simple discrimination design\n",
    "\n",
    "- Consider White, Nathan, and Faller (2015), which seeks to measure discrimination against Latinos by election officials through assessing whether election officials respond to emailed requests for information from Latino or White voters.\n",
    "\n",
    "- Discriminators are defined by their behavior: they would respond to the White voter but not to the Latino voter. \n",
    "\n",
    "- We imagine three types of election officials: those who would always respond to the request (regardless of the emailer’s ethnicity), those who would never respond to the request (again regardless of the emailer’s ethnicity), and officials who discriminate against Latinos. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b636f76",
   "metadata": {},
   "source": [
    "| Type                      | Yi(Zi=White) | Yi(Zi=Latino) |\n",
    "|---------------------------|--------------|---------------|\n",
    "| Always-responder          | 1            | 1             |\n",
    "| Anti-Latino discriminator | 1            | 0             |\n",
    "| Never-responder           | 0            | 0             |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5471316a",
   "metadata": {},
   "source": [
    "The inquiry here is descriptive: *the fraction of the sample that discriminates*: \n",
    "$$\n",
    "\\mathbb{E}[\\textrm{Type}_i = \\textrm{Anti}~\\textrm{Latino}~\\textrm{discriminator}]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f5d0a3",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b1ff0c5",
   "metadata": {},
   "source": [
    "## Simple design consistent with [Christensen et al. (2021)](https://www.nber.org/system/files/working_papers/w29516/w29516.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c984f6e2",
   "metadata": {},
   "source": [
    "<div >\n",
    "    <img src = \"figs/christensen1.png\" style=\"width:600px;height:400px;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6586e92",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "panels <- fabricate(\n",
    "  listings = add_level(N = 500, listing_fe = runif(N, -.1, .1)),\n",
    "  days = add_level(N = 2, day_shock = runif(N, -.05, .05), nest = FALSE),\n",
    "  obs = cross_levels(\n",
    "    by = join_using(listings, days),\n",
    "    U =rnorm(N, 0, .1),\n",
    "    epsilon = listing_fe + day_shock + U\n",
    "  )\n",
    "  \n",
    "  \n",
    ")\n",
    "\n",
    "require(tidyverse)\n",
    "panels  %>% arrange(listings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7293ed3a",
   "metadata": {},
   "source": [
    "<div >\n",
    "    <img src = \"figs/christensen2.png\" style=\"height:100px;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f2c6f3",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "design0<-declare_model(panels,\n",
    "                       potential_outcomes(Y ~ rbinom(n = N, size = 1, prob = 0.6-0.05* Z+epsilon)))+ NULL\n",
    "\n",
    "head(draw_data(design0) %>% arrange(listings),20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3302dbd5",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "design0<-declare_model(panels,\n",
    "                       potential_outcomes(Y ~ rbinom(n = N, size = 1, prob = 0.6-0.05* Z+epsilon)))+\n",
    "                      declare_inquiry(ATE=mean(Y_Z_1-Y_Z_0))  + declare_assignment(Z=block_ra(blocks=listings)) + \n",
    "                      declare_measurement(Y=reveal_outcomes(Y~Z))+\n",
    "                      declare_estimator(Y~Z,term=\"Z\", .method=lm, label=\"OLS\") +\n",
    "                        declare_estimator(Y~Z+factor(listings)+factor(days),term=\"Z\", .method=lm, label=\"FE\") +\n",
    "                        declare_estimator(Y~Z,term=\"Z\", .method=glm, family=\"binomial\", label=\"Logit\")\n",
    "diagnose_design(design0,sims=30)\n",
    "#head(draw_data(design0) %>% arrange(listings),20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b185739",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "p_load(\"margins\") # for margins\n",
    "p_load(\"broom\") # for tidy\n",
    "\n",
    "tidy_margins <- function(x) {\n",
    "  tidy(margins(x, data = x$data), conf.int = TRUE)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc11977d",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "design1<-declare_model(panels,\n",
    "                       potential_outcomes(Y ~ rbinom(n = N, size = 1, prob = 0.6-0.05* Z+epsilon)))+\n",
    "  declare_inquiry(ATE=mean(Y_Z_1-Y_Z_0)) +\n",
    "  declare_assignment(Z=block_ra(blocks=listings)) + \n",
    "  declare_measurement(Y=reveal_outcomes(Y~Z))+\n",
    "  declare_estimator(Y~Z+factor(listings)+factor(days),term=\"Z\", .method=lm, label=\"FE\") +\n",
    "  declare_estimator(Y~Z,term=\"Z\", .method=lm, label=\"OLS\") +\n",
    "  declare_estimator(Y~Z,term=\"Z\", .method=glm, family=\"binomial\", label=\"Logit\", .summary = tidy_margins)\n",
    "\n",
    "\n",
    "\n",
    "diagnose_design(design1,sims=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410b2cc8",
   "metadata": {},
   "source": [
    "## Simple design consistent with [Christensen et al. (2022)](https://direct.mit.edu/rest/article-abstract/104/4/807/97712/Housing-Discrimination-and-the-Toxics-Exposure-Gap?redirectedFrom=fulltext)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07fb6e7",
   "metadata": {},
   "source": [
    "<div >\n",
    "    <img src = \"figs/fig4_ba.png\" style=\"height:500px;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82b4ce2",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "panels <- fabricate(\n",
    "  listings = add_level(N = 500, listing_fe = runif(N, -.1, .1), more_than_mile = rbinom(N,size=1,prob=0.3)),\n",
    "  days = add_level(N = 2, day_shock = runif(N, -.05, .05), nest = FALSE),\n",
    "  obs = cross_levels(\n",
    "    by = join_using(listings, days),\n",
    "    U =rnorm(N, 0, .01),\n",
    "    epsilon = listing_fe + day_shock + U\n",
    "  )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28451ee",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "design0<-declare_model(panels,\n",
    "                       potential_outcomes(Y ~ rbinom(n = N, size = 1, prob = 0.4-0.1* Z -0.08*Z*more_than_mile+epsilon)))+\n",
    "  declare_inquiry(CATE_X1 =mean(Y_Z_1[more_than_mile == 1] - Y_Z_0[ more_than_mile== 1])  ,\n",
    "                  CATE_X0 = mean(Y_Z_1[more_than_mile == 0] - Y_Z_0[more_than_mile == 0]),\n",
    "                  diff_in_CATEs = CATE_X1- CATE_X0) +\n",
    "  declare_assignment(Z=block_ra(blocks=listings)) + \n",
    "  declare_measurement(Y=reveal_outcomes(Y~Z))+\n",
    "  declare_estimator(Y~Z + more_than_mile + Z * more_than_mile,term=\"Z\", .method=lm, label=\"within_mile\", inquiry=\"CATE_X0\") +\n",
    "  declare_estimator(Y ~ Z + more_than_mile + Z * more_than_mile, \n",
    "                    .method=lm,\n",
    "                    term = \"Z:more_than_mile\", \n",
    "                    inquiry = \"diff_in_CATEs\")\n",
    "\n",
    "\n",
    "diagnose_design(design0,sims=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74932336",
   "metadata": {},
   "source": [
    "## Key takeaways for the DeclareDesign workflow\n",
    "\n",
    "- DeclareDesign forces us to make each component explicit: model, inquiry, data strategy, and answer strategy.\n",
    "- Analytical formulas are fast, but simulations let us accommodate spillovers, heterogeneous treatment effects, and clustered assignments common in urban economics projects.\n",
    "- Diagnosing designs early reveals whether we need more units, better measures, or different estimators before committing resources to data collection.\n",
    "- Reproducing the diagnosis (set seeds, save scripts) keeps the dialogue between analysts and decision-makers focused on evidence rather than guesswork."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a6f2ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,Rmd"
  },
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.5.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
